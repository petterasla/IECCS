########################################################################################
MultinomialNB(alpha=0.1, class_prior=None, fit_prior=False)
########################################################################################
================================================================================
Cross validated train score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.1935    0.1463    0.1667        41
      FAVOR     0.5697    0.7634    0.6525      2046
       NONE     0.8559    0.7155    0.7794      4183

avg / total     0.7581    0.7274    0.7340      6270

macro-average of F-score(FAVOR), F-score(AGAINST) and F-score (NONE): 0.5329


Start validation. Time used: 1.3 minutes
================================================================================
Validation score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.2143    0.1765    0.1935        17
      FAVOR     0.5755    0.7868    0.6647       877
       NONE     0.8731    0.7178    0.7879      1793

avg / total     0.7718    0.7369    0.7439      2687

Validation macro F-score: 0.5487


================================================================================
TEST RESULTS FOR
MultinomialNB(alpha=0.1, class_prior=None, fit_prior=False)
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.3125    0.2632    0.2857        19
      FAVOR     0.5958    0.7862    0.6779       973
       NONE     0.8752    0.7398    0.8019      1991

avg / total     0.7805    0.7519    0.7581      2983

Test macro F-score: 0.5885
Time used 1.9 in minutes


########################################################################################
BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)
########################################################################################
================================================================================
Cross validated train score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.1385    0.2195    0.1698        41
      FAVOR     0.5991    0.7458    0.6645      2046
       NONE     0.8546    0.7473    0.7973      4183

avg / total     0.7665    0.7434    0.7499      6270

macro-average of F-score(FAVOR), F-score(AGAINST) and F-score(NONE): 0.5439

================================================================================
Validation score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.0500    0.0588    0.0541        17
      FAVOR     0.6277    0.7035    0.6634       877
       NONE     0.8450    0.7936    0.8185      1793

avg / total     0.7690    0.7596    0.7631      2687

Validation macro F-score: 0.5120


================================================================================
TEST RESULTS FOR
BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.0000    0.0000    0.0000        19
      FAVOR     0.6721    0.6763    0.6742       973
       NONE     0.8377    0.8428    0.8403      1991

avg / total     0.7784    0.7831    0.7807      2983

Test macro F-score: 0.5048
time =  190.241985083




########################################################################################
LinearSVC(C=5.2, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)
########################################################################################
================================================================================
Cross validated train score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.5000    0.0244    0.0465        41
      FAVOR     0.6561    0.6647    0.6604      2046
       NONE     0.8308    0.8331    0.8319      4183

avg / total     0.7716    0.7729    0.7708      6270

macro-average of F-score(FAVOR), F-score(AGAINST) and F-score(NONE): 0.5129

================================================================================
Validation score
================================================================================
/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
             precision    recall  f1-score   support

    AGAINST     0.0000    0.0000    0.0000        17
      FAVOR     0.6790    0.6682    0.6736       877
       NONE     0.8366    0.8511    0.8438      1793

avg / total     0.7799    0.7860    0.7829      2687

/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
Validation macro F-score: 0.5058


================================================================================
TEST RESULTS FOR
LinearSVC(C=5.2, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.4000    0.1053    0.1667        19
      FAVOR     0.6797    0.6259    0.6517       973
       NONE     0.8213    0.8589    0.8397      1991

avg / total     0.7724    0.7781    0.7741      2983

Test macro F-score: 0.5527
time =  139.873800039




########################################################################################
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='squared_hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)
########################################################################################

Cross validated train score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.1364    0.0732    0.0952        41
      FAVOR     0.6291    0.5860    0.6068      2046
       NONE     0.8010    0.8315    0.8160      4183

avg / total     0.7406    0.7464    0.7430      6270

macro-average of F-score(FAVOR), F-score(AGAINST) and F-score(NONE): 0.5060

================================================================================
Validation score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.1875    0.1765    0.1818        17
      FAVOR     0.6574    0.6716    0.6644       877
       NONE     0.8383    0.8299    0.8341      1793

avg / total     0.7751    0.7741    0.7746      2687

Validation macro F-score: 0.5601


================================================================================
TEST RESULTS FOR
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='squared_hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.0556    0.0526    0.0541        19
      FAVOR     0.6331    0.6136    0.6232       973
       NONE     0.8136    0.8262    0.8198      1991

avg / total     0.7499    0.7519    0.7508      2983

Test macro F-score: 0.4990
time =  221.301043987




================================================================================
SVC(C=5.2, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
================================================================================
Cross validated train score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.4615    0.1463    0.2222        41
      FAVOR     0.6355    0.6075    0.6212      2046
       NONE     0.8089    0.8317    0.8201      4183

avg / total     0.7500    0.7541    0.7513      6270

macro-average of F-score(FAVOR), F-score(AGAINST) and F-score(NONE): 0.5545

================================================================================
Validation score
================================================================================
             precision    recall  f1-score   support

    AGAINST     1.0000    0.1765    0.3000        17
      FAVOR     0.6545    0.6351    0.6447       877
       NONE     0.8211    0.8394    0.8301      1793

avg / total     0.7678    0.7685    0.7662      2687

Validation macro F-score: 0.5916


================================================================================
TEST RESULTS FOR
SVC(C=5.2, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.2500    0.0526    0.0870        19
      FAVOR     0.6670    0.6300    0.6480       973
       NONE     0.8214    0.8498    0.8353      1991

avg / total     0.7674    0.7730    0.7695      2983

Test macro F-score: 0.5234
time =  431.983071089





================================================================================
LogisticRegression(C=22.7, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=0, warm_start=False)
================================================================================
Cross validated train score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.0000    0.0000    0.0000        41
      FAVOR     0.6557    0.7214    0.6870      2046
       NONE     0.8522    0.8186    0.8350      4183

avg / total     0.7825    0.7815    0.7813      6270

macro-average of F-score(FAVOR), F-score(AGAINST) and F-score(NONE): 0.5073

================================================================================
Validation score
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.0000    0.0000    0.0000        17
      FAVOR     0.6980    0.7035    0.7007       877
       NONE     0.8525    0.8572    0.8548      1793

avg / total     0.7966    0.8016    0.7991      2687

Validation macro F-score: 0.5185


================================================================================
TEST RESULTS FOR
LogisticRegression(C=22.7, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=0, warm_start=False)
================================================================================
             precision    recall  f1-score   support

    AGAINST     0.0000    0.0000    0.0000        19
      FAVOR     0.7196    0.6146    0.6630       973
       NONE     0.8210    0.8870    0.8527      1991

avg / total     0.7827    0.7925    0.7854      2983

Test macro F-score: 0.5052
time =  821.145388842